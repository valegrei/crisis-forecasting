{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../libs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import shift_data, print_hp,print_line, graficarTodo, split_df, graficarClases, plot_metrics, plot_cm, plot_roc, plot_prc, plot_probs\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import datetime\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperModel\n",
    "import keras.metrics as metrics\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/PERU_DATA_DIFF.csv')\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.set_index('DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficarTodo(df,'Features')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mostrar Imbalance de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(df['Class'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graficarClases(df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividir Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['Class']\n",
    "features = df.columns[(df.columns!=target_col[0])]\n",
    "df_train, df_test = split_df(df,0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = df_train[features].copy()\n",
    "df_y_train = df_train[target_col].copy()\n",
    "df_x_test = df_test[features].copy()\n",
    "df_y_test = df_test[target_col].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_x_train.iloc[:,:] = scaler.fit_transform(df_x_train)\n",
    "df_x_test.iloc[:,:] = scaler.transform(df_x_test)\n",
    "\n",
    "df_x_train.iloc[:,:] = np.clip(df_x_train,-5,5)\n",
    "df_x_test.iloc[:,:] = np.clip(df_x_test,-5,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construccion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      metrics.TruePositives(name='tp'),\n",
    "      metrics.FalsePositives(name='fp'),\n",
    "      metrics.TrueNegatives(name='tn'),\n",
    "      metrics.FalseNegatives(name='fn'), \n",
    "      metrics.BinaryAccuracy(name='accuracy'),\n",
    "      metrics.Precision(name='precision'),\n",
    "      metrics.Recall(name='recall'),\n",
    "      metrics.AUC(name='auc'),\n",
    "      metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "def build_cnn(n_steps_in: int, n_features: int, conv1_kernels: int,\n",
    "              conv1_kernel_size: int, conv1_dropout : float, conv2_kernels: int,\n",
    "              conv2_kernel_size: int, conv2_dropout : float, dense_nodes: int, \n",
    "              dense_layers : int, dense_dropout : float, learning_rate: float, \n",
    "              conv_activation = None, dense_activation = None,\n",
    "              metrics = METRICS, output_bias = None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = Constant(output_bias)\n",
    "    model = Sequential()\n",
    "    # Conv 1\n",
    "    model.add(Conv1D(\n",
    "        filters=conv1_kernels,\n",
    "        kernel_size=conv1_kernel_size,\n",
    "        activation=conv_activation, \n",
    "        padding = 'same',\n",
    "        input_shape=(n_steps_in, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=conv1_kernel_size, padding='same'))\n",
    "    model.add(Dropout(conv1_dropout))\n",
    "    # Conv 2\n",
    "    model.add(Conv1D(\n",
    "        filters=conv2_kernels,\n",
    "        kernel_size=conv2_kernel_size,\n",
    "        activation=conv_activation, \n",
    "        padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size=conv2_kernel_size, padding='same'))\n",
    "    model.add(Dropout(conv2_dropout))\n",
    "    # Dense\n",
    "    model.add(Flatten())\n",
    "    for i in range(dense_layers):\n",
    "        model.add(Dense(dense_nodes, activation=dense_activation))\n",
    "    model.add(Dropout(dense_dropout))\n",
    "    model.add(Dense(1, activation='sigmoid', bias_initializer=output_bias))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss=BinaryCrossentropy(), metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, n_features_in,n_steps_out, output_bias = None, class_weight= None, name = None, tunable = True):\n",
    "        super().__init__(name=name, tunable=tunable)\n",
    "        self.n_features = n_features_in\n",
    "        self.n_steps_out = n_steps_out\n",
    "        self.output_bias = output_bias\n",
    "        self.class_weight = class_weight\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Parametrizamos nro de capas, nro de nodos y ratio de aprendizaje\n",
    "        hp_time_steps = hp.Int('steps_in',4,24,step=1)\n",
    "        hp_conv1_kernels = hp.Int('conv1_kernels',32,356,step=16)\n",
    "        hp_conv1_kernel_size = hp.Int('conv1_kernel_size',2,5,step=1)\n",
    "        hp_conv1_dropout = hp.Float('conv1_dropout',0.1,0.5,step=0.1)\n",
    "        hp_conv2_kernels = hp.Int('conv2_kernels',32,356,step=16)\n",
    "        hp_conv2_kernel_size = hp.Int('conv2_kernel_size',2,5,step=1)\n",
    "        hp_conv2_dropout = hp.Float('conv2_dropout',0.1,0.5,step=0.1)\n",
    "        hp_dense_nodes = hp.Int('dense_nodes',32,356,step=16)\n",
    "        hp_dense_layers = hp.Int('dense_layers',1,5,step=1)\n",
    "        hp_dense_dropout = hp.Float('dense_dropout',0.1,0.5,step=0.1)\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        hp_conv_activation = hp.Choice('conv_activation', values=['relu','tanh'])\n",
    "        hp_dense_activation = hp.Choice('dense_activation', values=['relu','tanh'])\n",
    "\n",
    "        return build_cnn(\n",
    "            n_steps_in = hp_time_steps,\n",
    "            n_features = self.n_features,\n",
    "            conv1_kernels = hp_conv1_kernels,\n",
    "            conv1_kernel_size = hp_conv1_kernel_size,\n",
    "            conv1_dropout = hp_conv1_dropout,\n",
    "            conv2_kernels = hp_conv2_kernels,\n",
    "            conv2_kernel_size = hp_conv2_kernel_size,\n",
    "            conv2_dropout = hp_conv2_dropout,\n",
    "            dense_nodes = hp_dense_nodes,\n",
    "            dense_layers = hp_dense_layers,\n",
    "            dense_dropout = hp_dense_dropout,\n",
    "            learning_rate = hp_learning_rate,\n",
    "            conv_activation = hp_conv_activation,\n",
    "            dense_activation = hp_dense_activation\n",
    "        )\n",
    "\n",
    "    def fit(self, hp, model,x,y,**kwargs):\n",
    "        x_s,y_s = shift_data(x, y, hp.get('steps_in'), self.n_steps_out)\n",
    "        #mini_batch = 32\n",
    "        batch_learning = len(x_s)\n",
    "        return model.fit(x = x_s, y = y_s, batch_size = batch_learning, class_weight=self.class_weight, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_out = [1,6,12,24]\n",
    "n_features = len(features)\n",
    "ajuste_path = os.path.normpath('G:/')\n",
    "fecha_hora = datetime.datetime.now().strftime('%Y%m%d_%H%M')\n",
    "objective = kt.Objective('val_prc',direction='max')\n",
    "max_epochs = 30\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'ajustes/'\n",
    "output_path_model = 'modelos/'\n",
    "name_prj = 'CNN_USA_'\n",
    "N = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizar_y_entrenar(name_prj, n_steps_out):\n",
    "    name_prj = name_prj + str(n_steps_out) +'_'+fecha_hora\n",
    "    name_model_tun = output_path+name_prj+'.h5'\n",
    "    # Condicion de parada: 10 epocas despues del menor val_loss\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_prc', \n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        restore_best_weights=True)\n",
    "    \n",
    "    # --------------------- Optimizacion --------------------------------------\n",
    "    cnn_hypermodel = CNNHyperModel(n_features, n_steps_out, class_weight = class_weight) # output_bias=initial_bias\n",
    "\n",
    "    cnn_tuner = kt.Hyperband(\n",
    "        cnn_hypermodel,\n",
    "        objective = objective,\n",
    "        max_epochs = max_epochs,\n",
    "        #directory = ajuste_path,\n",
    "        project_name = name_prj,\n",
    "        overwrite=True)\n",
    "\n",
    "    print(\"Optimizando...\")\n",
    "    cnn_tuner.search(x = df_x_train, y = df_y_train, validation_split = 0.5, epochs = n_epochs\n",
    "        , verbose = 0, shuffle = False, callbacks = [es])\n",
    "\n",
    "    # guardar parametros de mejor modelo\n",
    "    best_cnn_hps = cnn_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "    print_hp(output_path+name_prj+'.txt',cnn_tuner)\n",
    "\n",
    "    cnn_model = cnn_tuner.hypermodel.build(best_cnn_hps)\n",
    "    cnn_model.save(name_model_tun) # modelo sin entrenar\n",
    "    print(best_cnn_hps.values)\n",
    "\n",
    "    # --------------------- Entrenamiento --------------------------------------\n",
    "    n_steps_in = best_cnn_hps.get('steps_in')\n",
    "    x_train, y_train = shift_data(df_x_train, df_y_train, n_steps_in, n_steps_out)\n",
    "    x_test, y_test = shift_data(df_x_test, df_y_test, n_steps_in, n_steps_out)\n",
    "    batch_learning = len(x_train)\n",
    "\n",
    "    prcs = []\n",
    "    models = []\n",
    "    res_path = output_path_model+name_prj+\".csv\"\n",
    "    print_line(\"loss,tp,fp,tn,fn,accuracy,precision,recall,auc,prc\\n\",res_path)\n",
    "    print(\"Entrenando...\")\n",
    "\n",
    "    for i in range(N):\n",
    "        name_model = output_path_model+name_prj+'_'+str(i)+'.h5'\n",
    "        # Condicion de parada: 10 epocas despues del menor val_loss\n",
    "        es_t = EarlyStopping(\n",
    "            monitor='val_prc', \n",
    "            verbose=1,\n",
    "            patience=10,\n",
    "            mode='max',\n",
    "            restore_best_weights=True)\n",
    "        models.append(name_model)\n",
    "        history = cnn_model.fit(x=x_train, y=y_train, validation_split = 0.5, epochs = n_epochs\n",
    "            , verbose = 0, shuffle = False, callbacks = [es_t], batch_size = batch_learning, class_weight=class_weight)\n",
    "\n",
    "        cnn_model.save(name_model) \n",
    "        # --------------------- Evaluacion --------------------------------------\n",
    "        res = cnn_model.evaluate(x_test, y_test,\n",
    "                                        batch_size=batch_learning, verbose=0)\n",
    "        print_line(f'{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]},{res[6]},{res[7]},{res[8]},{res[9]}\\n', res_path)\n",
    "        prcs.append(res[9]) #guarda metrica a comparar\n",
    "\n",
    "    #--------- Evaluando el mejor ------------------------\n",
    "    print(\"Evaluando...\")\n",
    "    best_model = models[np.argmax(prcs)]\n",
    "    print(f'\\nMejor modelo: {best_model} con prc: {np.max(prcs)}\\n')\n",
    "\n",
    "    cnn_model = load_model(best_model)\n",
    "    train_predictions = cnn_model(x_train) #, batch_size=batch_learning\n",
    "    test_predictions = cnn_model(x_test)\n",
    "\n",
    "    plot_cm(y_test, test_predictions)\n",
    "    plot_roc(y_train, train_predictions, y_test, test_predictions)\n",
    "    plot_prc(y_train, train_predictions, y_test, test_predictions)\n",
    "    plot_probs(y_train, train_predictions, 'Train')\n",
    "    plot_probs(y_test, test_predictions, 'Test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo para 1 mes a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizar_y_entrenar(name_prj, n_steps_out[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo para 6 meses a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizar_y_entrenar(name_prj, n_steps_out[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo para 12 meses a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizar_y_entrenar(name_prj, n_steps_out[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo para 24 meses a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizar_y_entrenar(name_prj, n_steps_out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sys.path.append(\"../libs/\")\n",
    "# from notifications import enviar_correo\n",
    "# enviar_correo(\"Ajuste de Parametros Finalizado!\",\"Se ha completado: {}\".format(name_prj))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6822839c80bb80c42f7f9e096efdd447a89633a8e8a553b5cfb2012f3a4eafe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
